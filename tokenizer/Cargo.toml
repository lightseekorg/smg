[package]
name = "tokenizer"
version = "0.1.0"
edition = "2021"
description = "LLM tokenizer library with caching and chat template support"
license = "Apache-2.0"
repository = "https://github.com/lightseekorg/smg"
authors = [
    "Simo Lin <linsimo.mark@gmail.com>",
    "Chang Su <mckvtl@gmail.com>",
    "Keyang Ru <rukeyang@gmail.com>",
]
keywords = ["tokenizer", "llm", "huggingface", "tiktoken", "chat-template"]
categories = ["text-processing", "parsing"]

[lib]
name = "llm_tokenizer"

[dependencies]
anyhow.workspace = true
blake3.workspace = true
bytemuck = { workspace = true, features = ["derive"] }
dashmap.workspace = true
lru.workspace = true
parking_lot.workspace = true
serde = { workspace = true, features = ["derive"] }
serde_json.workspace = true
thiserror.workspace = true
tokio = { workspace = true, features = ["sync", "rt-multi-thread", "macros"] }
tracing.workspace = true
uuid = { workspace = true, features = ["v4", "serde"] }
# Only used by tokenizer
hf-hub = { version = "0.4.3", features = ["tokio"] }
minijinja = { version = "2.0", features = ["unstable_machinery", "json", "builtins"] }
minijinja-contrib = { version = "2.0", features = ["pycompat"] }
rayon = "1.10"
tiktoken-rs = "0.7.0"
tokenizers = "0.22.0"

[dev-dependencies]
openai-protocol.workspace = true
reqwest = { workspace = true, features = ["blocking"] }
tempfile = "3.8"
