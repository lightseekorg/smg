name: PR Test (SMG)

on:
  push:
    branches: [ main ]
    paths-ignore:
      - "docs/**"
      - "*.md"
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened, labeled]
    paths-ignore:
      - "docs/**"
      - "*.md"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: gateway-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  RUSTC_WRAPPER: sccache
  SCCACHE_GHA_ENABLED: "true"

jobs:
  check-ci:
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - uses: actions/checkout@v6
        with:
          sparse-checkout: .github/actions/check-ci
      - id: check
        uses: ./.github/actions/check-ci

  determine-stages:
    runs-on: k8s-runner-cpu
    outputs:
      run_smoke: ${{ steps.check.outputs.run_smoke }}
      run_e2e: ${{ steps.check.outputs.run_e2e }}
      run_benchmark: ${{ steps.check.outputs.run_benchmark }}
    steps:
      - name: Check stage labels
        id: check
        run: |
          # Default: run all stages
          run_smoke="true"
          run_e2e="true"
          run_benchmark="true"

          # Check labels (only on pull_request events)
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-smoke') }}" == "true" ]]; then
              run_smoke="false"
              echo "Skipping smoke tests (ci:skip-smoke label)"
            fi
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-e2e') }}" == "true" ]]; then
              run_e2e="false"
              echo "Skipping e2e tests (ci:skip-e2e label)"
            fi
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-benchmark') }}" == "true" ]]; then
              run_benchmark="false"
              echo "Skipping benchmarks (ci:skip-benchmark label)"
            fi
          fi

          echo "run_smoke=$run_smoke" >> $GITHUB_OUTPUT
          echo "run_e2e=$run_e2e" >> $GITHUB_OUTPUT
          echo "run_benchmark=$run_benchmark" >> $GITHUB_OUTPUT

  build-wheel:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Cache wheel and Go FFI artifacts
        id: cache-wheel
        uses: actions/cache@v4
        with:
          path: |
            bindings/python/dist/*.whl
            bindings/golang/target/release/libsmg_go.*
          key: wheel-${{ runner.os }}-${{ hashFiles('Cargo.lock', '**/Cargo.toml', 'auth/src/**', 'bindings/python/src/**', 'data_connector/src/**', 'grpc_client/src/**', 'kv_index/src/**', 'mcp/src/**', 'mesh/src/**', 'model_gateway/src/**', 'multimodal/src/**', 'protocols/src/**', 'reasoning_parser/src/**', 'tokenizer/src/**', 'tool_parser/src/**', 'wasm/src/**', 'workflow/src/**', 'bindings/golang/src/**') }}
          restore-keys: |
            wheel-${{ runner.os }}-

      - name: Setup Rust
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-rust

      - name: Build Python wheel and Go FFI library
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        run: |
          bash scripts/ci_setup_python_venv.sh
          bash scripts/ci_build_wheel.sh

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v6
        with:
          name: smg-wheel
          path: bindings/python/dist/*.whl
          retention-days: 1

      - name: Upload Go FFI library artifact
        uses: actions/upload-artifact@v6
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/libsmg_go.*
          retention-days: 1

      - name: Show sccache stats
        if: always() && steps.cache-wheel.outputs.cache-hit != 'true'
        run: sccache --show-stats

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"

      - name: Test wheel install
        run: |
          pip install bindings/python/dist/*.whl
          python3 -c "import smg; print('Python package: OK')"
          python3 -c "from smg.smg_rs import Router; print('Rust extension: OK')"
          python3 -m smg.launch_router --help > /dev/null && echo "Entry point: OK"

  python-unit-tests:
    needs: build-wheel
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: dist/

      - name: Install wheel
        run: pip install dist/*.whl

      - name: Run Python unit tests
        run: |
          cd bindings/python
          python3 -m pip install pytest pytest-cov pytest-xdist
          pytest -q tests --cov=smg --cov-config=.coveragerc --cov-report=term-missing --cov-fail-under=80

  unit-tests:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust

      - name: Run lint
        run: |
          source "$HOME/.cargo/env"
          rustup component add clippy
          cargo clippy --all-targets --all-features -- -D warnings

      - name: Run fmt
        run: |
          source "$HOME/.cargo/env"
          rustup component add --toolchain nightly-x86_64-unknown-linux-gnu rustfmt
          rustup toolchain install nightly --profile minimal
          cargo +nightly fmt -- --check

      - name: Generate vision golden fixtures
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

          pip install transformers pillow numpy scipy
          python multimodal/scripts/generate_vision_golden.py

      - name: Run Rust tests
        timeout-minutes: 30
        run: |
          source "$HOME/.cargo/env"
          cargo test

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats

  # =============================================================================
  # Stage 1: Smoke Tests (quick sanity checks)
  # =============================================================================
  gateway-smoke:
    name: ${{ matrix.name }}
    needs: [check-ci, build-wheel, determine-stages]
    if: needs.check-ci.outputs.should_run == 'true' && needs.determine-stages.outputs.run_smoke == 'true'
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: gw-sgl-smoke-test
            timeout: 45
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"
            env_vars: "SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
          - name: gw-vllm-smoke-test
            timeout: 45
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            setup_vllm: true
            # vLLM: gRPC only, PD mode supported
            # HTTP tests auto-skipped via @pytest.mark.skip_for_runtime("vllm")
          - name: gw-trtllm-smoke-test
            timeout: 90
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            setup_trtllm: true
            # TRT-LLM: gRPC only, no PD mode
            # HTTP and PD tests auto-skipped via markers
    runs-on: k8s-runner-gpu
    timeout-minutes: ${{ matrix.timeout }}
    steps:
      - uses: actions/checkout@v6
      - name: Run gateway tests
        uses: ./.github/actions/gateway-test
        with:
          test_dirs: ${{ matrix.test_dirs }}
          extra_deps: ${{ matrix.extra_deps }}
          env_vars: ${{ matrix.env_vars }}
          reruns: ${{ matrix.reruns }}
          parallel_opts: ${{ matrix.parallel_opts }}
          setup_vllm: ${{ matrix.setup_vllm || 'false' }}
          setup_trtllm: ${{ matrix.setup_trtllm || 'false' }}
          hf_token: ${{ secrets.HF_TOKEN }}

  # =============================================================================
  # Stage 2: E2E Tests (comprehensive testing) - runs after smoke tests pass
  # =============================================================================
  gateway-e2e:
    name: ${{ matrix.name }}
    needs: [check-ci, build-wheel, determine-stages, gateway-smoke]
    if: |
      always() &&
      needs.check-ci.outputs.should_run == 'true' &&
      needs.determine-stages.outputs.run_e2e == 'true' &&
      (needs.gateway-smoke.result == 'success' || needs.gateway-smoke.result == 'skipped')
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: chat-completions-sglang
            timeout: 45
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=sglang SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
          - name: chat-completions-vllm
            timeout: 45
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            # TODO: Remove filter when vLLM supports logprobs and n>1 with greedy sampling
            test_filter: "-k 'not (5-grpc or 2-None-grpc or multiple_choices)'"
            setup_vllm: true
          - name: chat-completions-trtllm
            timeout: 90
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            setup_trtllm: true
          - name: responses
            timeout: 45
            test_dirs: "e2e_test/responses"
            env_vars: "SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            setup_oracle: true
            setup_brave: true
    runs-on: k8s-runner-gpu
    timeout-minutes: ${{ matrix.timeout }}
    steps:
      - uses: actions/checkout@v6
      - name: Run gateway tests
        uses: ./.github/actions/gateway-test
        with:
          test_dirs: ${{ matrix.test_dirs }}
          extra_deps: ${{ matrix.extra_deps || '' }}
          env_vars: ${{ matrix.env_vars }}
          reruns: ${{ matrix.reruns }}
          parallel_opts: ${{ matrix.parallel_opts || '' }}
          test_filter: ${{ matrix.test_filter || '' }}
          setup_vllm: ${{ matrix.setup_vllm || 'false' }}
          setup_trtllm: ${{ matrix.setup_trtllm || 'false' }}
          setup_oracle: ${{ matrix.setup_oracle || 'false' }}
          setup_brave: ${{ matrix.setup_brave || 'false' }}
          hf_token: ${{ secrets.HF_TOKEN }}
          brave_api_key: ${{ secrets.BRAVE_API_KEY }}

  # =============================================================================
  # Stage 3: Benchmarks (performance) - runs after e2e tests pass
  # =============================================================================
  gateway-benchmark:
    name: benchmarks
    needs: [check-ci, build-wheel, determine-stages, gateway-e2e]
    if: |
      always() &&
      needs.check-ci.outputs.should_run == 'true' &&
      needs.determine-stages.outputs.run_benchmark == 'true' &&
      (needs.gateway-e2e.result == 'success' || needs.gateway-e2e.result == 'skipped')
    permissions:
      contents: read
    runs-on: k8s-runner-gpu
    timeout-minutes: 32
    steps:
      - uses: actions/checkout@v6
      - name: Run gateway tests
        uses: ./.github/actions/gateway-test
        with:
          test_dirs: "e2e_test/benchmarks"
          extra_deps: "genai-bench==0.0.3"
          ignore_opts: "--ignore=e2e_test/benchmarks/test_go_bindings_perf.py"
          upload_benchmarks: "true"
          hf_token: ${{ secrets.HF_TOKEN }}

  go-unit-tests:
    name: go-unit-tests
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Install build tools
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Run Go unit tests
        run: |
          cd bindings/golang
          export CGO_ENABLED=1
          export CGO_LDFLAGS="-L$(pwd)/target/release"
          export LD_LIBRARY_PATH="$(pwd)/target/release:$LD_LIBRARY_PATH"
          go test -v ./...

  go-bindings-e2e:
    name: go-bindings-e2e
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 45
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python venv and SGLang
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh

      - name: Run Go OAI server E2E tests
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 e2e_test/bindings_go -s -vv -o log_cli=true --log-cli-level=INFO

  go-bindings-benchmark:
    name: go-bindings-benchmark
    needs: [check-ci, build-wheel]
    if: false  # Disabled
    runs-on: k8s-runner-gpu
    timeout-minutes: 32
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install inference backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl

      - name: Install test dependencies
        run: |
          python3 -m pip install pytest pytest-rerunfailures httpx openai grpcio grpcio-health-checking numpy
          python3 -m pip --no-cache-dir install --upgrade genai-bench==0.0.3

      - name: Run Go bindings benchmark
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest e2e_test/benchmarks/test_go_bindings_perf.py -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: success()
        uses: actions/upload-artifact@v6
        with:
          name: genai-bench-results-go-bindings
          path: benchmark_go_bindings/

  finish:
    needs: [check-ci, determine-stages, build-wheel, python-unit-tests, unit-tests, gateway-smoke, gateway-e2e, gateway-benchmark, go-unit-tests, go-bindings-e2e]
    if: always()
    runs-on: k8s-runner-cpu
    permissions: {}
    steps:
      - name: Check CI result
        run: |
          if [[ "${{ needs.check-ci.outputs.should_run }}" != "true" ]]; then
            echo "CI was skipped (external contributor without run-ci label)"
            exit 0
          elif [[ "${{ needs.build-wheel.result }}" == "failure" || "${{ needs.unit-tests.result }}" == "failure" || "${{ needs.gateway-smoke.result }}" == "failure" || "${{ needs.gateway-e2e.result }}" == "failure" || "${{ needs.gateway-benchmark.result }}" == "failure" || "${{ needs.go-unit-tests.result }}" == "failure" || "${{ needs.go-bindings-e2e.result }}" == "failure" ]]; then
            echo "One or more jobs failed"
            exit 1
          else
            echo "All jobs completed successfully"
          fi

  summarize-benchmarks:
    needs: [gateway-benchmark]
    runs-on: k8s-runner-cpu
    if: success()
    permissions:
      contents: read
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: "3.13"

    - name: Download gateway benchmark results
      uses: actions/download-artifact@v7
      with:
        name: genai-bench-results-all-policies

    - name: Create benchmark summary
      run: python3 e2e_test/benchmarks/summarize.py .
