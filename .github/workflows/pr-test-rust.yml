name: PR Test (SMG)

on:
  push:
    branches: [ main ]
    paths-ignore:
      - "docs/**"
      - "*.md"
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened, labeled]
    paths-ignore:
      - "docs/**"
      - "*.md"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: gateway-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  RUSTC_WRAPPER: sccache
  SCCACHE_GHA_ENABLED: "true"

jobs:
  check-ci:
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - uses: actions/checkout@v6
        with:
          sparse-checkout: .github/actions/check-ci
      - id: check
        uses: ./.github/actions/check-ci

  determine-stages:
    runs-on: k8s-runner-cpu
    outputs:
      run_smoke: ${{ steps.check.outputs.run_smoke }}
      run_e2e: ${{ steps.check.outputs.run_e2e }}
      run_benchmark: ${{ steps.check.outputs.run_benchmark }}
    steps:
      - name: Check stage labels
        id: check
        run: |
          # Default: run all stages
          run_smoke="true"
          run_e2e="true"
          run_benchmark="true"

          # Check labels (only on pull_request events)
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-smoke') }}" == "true" ]]; then
              run_smoke="false"
              echo "Skipping smoke tests (ci:skip-smoke label)"
            fi
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-e2e') }}" == "true" ]]; then
              run_e2e="false"
              echo "Skipping e2e tests (ci:skip-e2e label)"
            fi
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'ci:skip-benchmark') }}" == "true" ]]; then
              run_benchmark="false"
              echo "Skipping benchmarks (ci:skip-benchmark label)"
            fi
          fi

          echo "run_smoke=$run_smoke" >> $GITHUB_OUTPUT
          echo "run_e2e=$run_e2e" >> $GITHUB_OUTPUT
          echo "run_benchmark=$run_benchmark" >> $GITHUB_OUTPUT

  build-wheel:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Cache wheel and Go FFI artifacts
        id: cache-wheel
        uses: actions/cache@v4
        with:
          path: |
            bindings/python/dist/*.whl
            bindings/golang/target/release/libsmg_go.*
          key: wheel-${{ runner.os }}-${{ hashFiles('Cargo.lock', '**/Cargo.toml', 'auth/src/**', 'bindings/python/src/**', 'data_connector/src/**', 'grpc_client/src/**', 'kv_index/src/**', 'mcp/src/**', 'mesh/src/**', 'model_gateway/src/**', 'multimodal/src/**', 'protocols/src/**', 'reasoning_parser/src/**', 'tokenizer/src/**', 'tool_parser/src/**', 'wasm/src/**', 'workflow/src/**', 'bindings/golang/src/**') }}
          restore-keys: |
            wheel-${{ runner.os }}-

      - name: Setup Rust
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-rust

      - name: Build Python wheel and Go FFI library
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        run: |
          bash scripts/ci_setup_python_venv.sh
          bash scripts/ci_build_wheel.sh

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v6
        with:
          name: smg-wheel
          path: bindings/python/dist/*.whl
          retention-days: 1

      - name: Upload Go FFI library artifact
        uses: actions/upload-artifact@v6
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/libsmg_go.*
          retention-days: 1

      - name: Show sccache stats
        if: always() && steps.cache-wheel.outputs.cache-hit != 'true'
        run: sccache --show-stats

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"

      - name: Test wheel install
        run: |
          pip install bindings/python/dist/*.whl
          python3 -c "import smg; print('Python package: OK')"
          python3 -c "from smg.smg_rs import Router; print('Rust extension: OK')"
          python3 -m smg.launch_router --help > /dev/null && echo "Entry point: OK"

  python-unit-tests:
    needs: build-wheel
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: dist/

      - name: Install wheel
        run: pip install dist/*.whl

      - name: Run Python unit tests
        run: |
          cd bindings/python
          python3 -m pip install pytest pytest-cov pytest-xdist
          pytest -q tests --cov=smg --cov-config=.coveragerc --cov-report=term-missing --cov-fail-under=80

  unit-tests:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust

      - name: Run lint
        run: |
          source "$HOME/.cargo/env"
          rustup component add clippy
          cargo clippy --all-targets --all-features -- -D warnings

      - name: Run fmt
        run: |
          source "$HOME/.cargo/env"
          rustup component add --toolchain nightly-x86_64-unknown-linux-gnu rustfmt
          rustup toolchain install nightly --profile minimal
          cargo +nightly fmt -- --check

      - name: Generate vision golden fixtures
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

          pip install transformers pillow numpy scipy
          python multimodal/scripts/generate_vision_golden.py

      - name: Run Rust tests
        timeout-minutes: 30
        run: |
          source "$HOME/.cargo/env"
          cargo test

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats

  gateway-e2e:
    name: ${{ matrix.name }}
    needs: [check-ci, build-wheel, determine-stages]
    if: needs.check-ci.outputs.should_run == 'true'
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        include:
          # Stage 1: Smoke Tests (quick sanity checks)
          - name: gw-sgl-smoke-test
            stage: smoke
            timeout: 45
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"  # py is required for pytest-parallel with newer pytest
            env_vars: "SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"  # Thread-based parallelism
            ignore_opts: ""
          - name: gw-vllm-smoke-test
            stage: smoke
            timeout: 45
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            test_filter: ""
            setup_vllm: true
            ignore_opts: ""
            # vLLM: gRPC only, PD mode supported
            # HTTP tests auto-skipped via @pytest.mark.skip_for_runtime("vllm")
          - name: gw-trtllm-smoke-test
            stage: smoke
            timeout: 90
            test_dirs: "e2e_test/router e2e_test/embeddings"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            test_filter: ""
            setup_trtllm: true
            ignore_opts: ""
            # TRT-LLM: gRPC only, no PD mode
            # HTTP and PD tests auto-skipped via markers

          # Stage 2: E2E Tests (comprehensive testing)
          - name: chat-completions-sglang
            stage: e2e
            timeout: 45
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=sglang SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            test_filter: ""
            ignore_opts: ""
          - name: chat-completions-vllm
            stage: e2e
            timeout: 45
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            # TODO: Remove filter when vLLM supports logprobs and n>1 with greedy sampling
            # Excludes: 5-grpc (logprobs=5), 2-None-grpc (n=2 with no logprobs), multiple_choices (n=2 tests)
            test_filter: "-k 'not (5-grpc or 2-None-grpc or multiple_choices)'"
            setup_vllm: true
            ignore_opts: ""
          - name: chat-completions-trtllm
            stage: e2e
            timeout: 90
            test_dirs: "e2e_test/chat_completions"
            extra_deps: "pytest-parallel py"
            env_vars: "E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            parallel_opts: "--workers 1 --tests-per-worker 4"
            test_filter: ""
            setup_trtllm: true
            ignore_opts: ""
          - name: responses
            stage: e2e
            timeout: 45
            test_dirs: "e2e_test/responses"
            extra_deps: ""
            env_vars: "SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1"
            reruns: "--reruns 2 --reruns-delay 5"
            setup_oracle: true
            setup_brave: true
            parallel_opts: ""  # Cloud backend tests not compatible with parallel execution
            ignore_opts: ""

          # Stage 3: Benchmarks (performance)
          - name: benchmarks
            stage: benchmark
            timeout: 32
            test_dirs: "e2e_test/benchmarks"
            extra_deps: "genai-bench==0.0.3"
            env_vars: ""
            reruns: ""
            upload_benchmarks: true
            parallel_opts: ""  # No parallel for benchmarks (performance measurement)
            ignore_opts: "--ignore=e2e_test/benchmarks/test_go_bindings_perf.py"  # Go benchmark runs in dedicated job
    runs-on: k8s-runner-gpu
    timeout-minutes: ${{ matrix.timeout }}
    steps:
      - name: Check if stage should run
        id: check-stage
        run: |
          should_run="true"
          if [[ "${{ matrix.stage }}" == "smoke" && "${{ needs.determine-stages.outputs.run_smoke }}" != "true" ]]; then
            should_run="false"
            echo "Skipping smoke test (ci:skip-smoke label)"
          elif [[ "${{ matrix.stage }}" == "e2e" && "${{ needs.determine-stages.outputs.run_e2e }}" != "true" ]]; then
            should_run="false"
            echo "Skipping e2e test (ci:skip-e2e label)"
          elif [[ "${{ matrix.stage }}" == "benchmark" && "${{ needs.determine-stages.outputs.run_benchmark }}" != "true" ]]; then
            should_run="false"
            echo "Skipping benchmark (ci:skip-benchmark label)"
          fi
          echo "should_run=$should_run" >> $GITHUB_OUTPUT

      - name: Checkout code
        if: steps.check-stage.outputs.should_run == 'true'
        uses: actions/checkout@v6

      - name: Cache flash-attn wheel
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_vllm
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip/wheels
          key: flash-attn-${{ runner.os }}-${{ hashFiles('scripts/ci_install_vllm.sh') }}

      - name: Restore TRT-LLM wheel cache
        id: trtllm-cache
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_trtllm
        uses: actions/cache/restore@v4
        with:
          path: /tmp/trtllm-wheel
          key: trtllm-wheel-${{ runner.os }}-cuda13-v2

      - name: Install inference backend
        if: steps.check-stage.outputs.should_run == 'true'
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          if [ "${{ matrix.setup_vllm }}" == "true" ]; then
            bash scripts/ci_install_vllm.sh
          elif [ "${{ matrix.setup_trtllm }}" == "true" ]; then
            bash scripts/ci_install_trtllm.sh
          else
            bash scripts/ci_install_sglang.sh
          fi

      # Save TRT-LLM wheel immediately after build, before tests run
      # This ensures the cache is saved even if tests fail or job is cancelled
      - name: Save TRT-LLM wheel cache
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_trtllm && steps.trtllm-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: /tmp/trtllm-wheel
          key: trtllm-wheel-${{ runner.os }}-cuda13-v2

      - name: Setup Oracle Instant Client
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_oracle
        run: |
          sudo apt-get install -y unzip
          INSTANT_CLIENT_DIR="$HOME/instant-client"
          INSTANT_CLIENT_ZIP="instantclient-basic-linux.x64-23.9.0.25.07.zip"

          if [ ! -d "$INSTANT_CLIENT_DIR/instantclient_23_9" ]; then
            echo "Downloading Oracle Instant Client..."
            mkdir -p "$INSTANT_CLIENT_DIR"
            cd "$INSTANT_CLIENT_DIR"
            wget https://download.oracle.com/otn_software/linux/instantclient/2390000/$INSTANT_CLIENT_ZIP
            unzip $INSTANT_CLIENT_ZIP
            rm $INSTANT_CLIENT_ZIP
          else
            echo "Oracle Instant Client already exists, skipping download"
          fi

          echo "LD_LIBRARY_PATH=$HOME/instant-client/instantclient_23_9:$LD_LIBRARY_PATH" >> $GITHUB_ENV

      - name: Start Oracle Database
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_oracle
        run: |
          docker run -d -p 1521:1521 -e ORACLE_PASSWORD=oracle --name oracle-db gvenzl/oracle-xe:21-slim
          echo "Starting Oracle DB..."

          # Export Oracle connection environment variables
          echo "ATP_USER=system" >> $GITHUB_ENV
          echo "ATP_PASSWORD=oracle" >> $GITHUB_ENV
          echo "ATP_DSN=localhost:1521/XEPDB1" >> $GITHUB_ENV

      - name: Start Brave MCP Server
        if: steps.check-stage.outputs.should_run == 'true' && matrix.setup_brave
        env:
          BRAVE_API_KEY: ${{ secrets.BRAVE_API_KEY }}
        run: |
          docker run -d --rm \
            -p 8001:8080 \
            -e BRAVE_API_KEY \
            --name brave-search-server \
            shoofio/brave-search-mcp-sse:1.0.10
          echo "Starting Brave MCP Server..."
          sleep 2
          curl -f --max-time 1 http://localhost:8001/sse > /dev/null 2>&1 && echo "Brave MCP Server is healthy!" || echo "Brave MCP Server responded"

      - name: Download wheel artifact
        if: steps.check-stage.outputs.should_run == 'true'
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        if: steps.check-stage.outputs.should_run == 'true'
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh ${{ matrix.extra_deps }}

      - name: Run E2E tests
        if: steps.check-stage.outputs.should_run == 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          ${{ matrix.env_vars }} ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" pytest ${{ matrix.reruns }} ${{ matrix.parallel_opts }} ${{ matrix.ignore_opts }} ${{ matrix.test_dirs }} ${{ matrix.test_filter }} -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: steps.check-stage.outputs.should_run == 'true' && matrix.upload_benchmarks && success()
        uses: actions/upload-artifact@v6
        with:
          name: genai-bench-results-all-policies
          path: benchmark_**/

      - name: Cleanup Brave MCP Server
        if: always() && matrix.setup_brave
        run: |
          docker stop brave-search-server || true
          docker rm brave-search-server || true

      - name: Cleanup Oracle Database
        if: always() && matrix.setup_oracle
        run: |
          docker stop oracle-db || true
          docker rm oracle-db || true

  go-unit-tests:
    name: go-unit-tests
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Install build tools
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Run Go unit tests
        run: |
          cd bindings/golang
          export CGO_ENABLED=1
          export CGO_LDFLAGS="-L$(pwd)/target/release"
          export LD_LIBRARY_PATH="$(pwd)/target/release:$LD_LIBRARY_PATH"
          go test -v ./...

  go-bindings-e2e:
    name: go-bindings-e2e
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 45
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python venv and SGLang
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh

      - name: Run Go OAI server E2E tests
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 e2e_test/bindings_go -s -vv -o log_cli=true --log-cli-level=INFO

  go-bindings-benchmark:
    name: go-bindings-benchmark
    needs: [check-ci, build-wheel]
    if: false  # Disabled
    runs-on: k8s-runner-gpu
    timeout-minutes: 32
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install inference backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl

      - name: Install test dependencies
        run: |
          python3 -m pip install pytest pytest-rerunfailures httpx openai grpcio grpcio-health-checking numpy
          python3 -m pip --no-cache-dir install --upgrade genai-bench==0.0.3

      - name: Run Go bindings benchmark
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest e2e_test/benchmarks/test_go_bindings_perf.py -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: success()
        uses: actions/upload-artifact@v6
        with:
          name: genai-bench-results-go-bindings
          path: benchmark_go_bindings/

  finish:
    needs: [check-ci, determine-stages, build-wheel, python-unit-tests, unit-tests, gateway-e2e, go-unit-tests, go-bindings-e2e]
    if: always()
    runs-on: k8s-runner-cpu
    permissions: {}
    steps:
      - name: Check CI result
        run: |
          if [[ "${{ needs.check-ci.outputs.should_run }}" != "true" ]]; then
            echo "CI was skipped (external contributor without run-ci label)"
            exit 0
          elif [[ "${{ needs.build-wheel.result }}" == "failure" || "${{ needs.unit-tests.result }}" == "failure" || "${{ needs.gateway-e2e.result }}" == "failure" || "${{ needs.go-unit-tests.result }}" == "failure" || "${{ needs.go-bindings-e2e.result }}" == "failure" ]]; then
            echo "One or more jobs failed"
            exit 1
          else
            echo "All jobs completed successfully"
          fi

  summarize-benchmarks:
    needs: [gateway-e2e]
    runs-on: k8s-runner-cpu
    if: success()
    permissions:
      contents: read
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: "3.13"

    - name: Download gateway benchmark results
      uses: actions/download-artifact@v7
      with:
        name: genai-bench-results-all-policies

    - name: Create benchmark summary
      run: python3 e2e_test/benchmarks/summarize.py .
