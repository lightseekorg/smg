name: PR Test (SMG)

on:
  push:
    branches: [ main ]
    paths-ignore:
      - "docs/**"
      - "*.md"
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened]
    paths-ignore:
      - "docs/**"
      - "*.md"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: gateway-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  RUSTC_WRAPPER: sccache
  SCCACHE_GHA_ENABLED: "true"

jobs:
  check-ci:
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - uses: actions/checkout@v6
        with:
          sparse-checkout: .github/actions/check-ci
      - id: check
        uses: ./.github/actions/check-ci

  build-wheel:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Cache wheel and Go FFI artifacts
        id: cache-wheel
        uses: actions/cache@v4
        with:
          path: |
            bindings/python/dist/*.whl
            bindings/golang/target/release/libsmg_go.*
          key: wheel-${{ runner.os }}-${{ hashFiles('Cargo.lock', '**/Cargo.toml', 'auth/src/**', 'bindings/python/src/**', 'data_connector/src/**', 'grpc_client/src/**', 'kv_index/src/**', 'mcp/src/**', 'mesh/src/**', 'model_gateway/src/**', 'multimodal/src/**', 'protocols/src/**', 'reasoning_parser/src/**', 'tokenizer/src/**', 'tool_parser/src/**', 'wasm/src/**', 'workflow/src/**', 'bindings/golang/src/**') }}
          restore-keys: |
            wheel-${{ runner.os }}-

      - name: Setup Rust
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-rust

      - name: Build Python wheel and Go FFI library
        if: steps.cache-wheel.outputs.cache-hit != 'true'
        run: |
          bash scripts/ci_setup_python_venv.sh
          bash scripts/ci_build_wheel.sh

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v6
        with:
          name: smg-wheel
          path: bindings/python/dist/*.whl
          retention-days: 1

      - name: Upload Go FFI library artifact
        uses: actions/upload-artifact@v6
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/libsmg_go.*
          retention-days: 1

      - name: Show sccache stats
        if: always() && steps.cache-wheel.outputs.cache-hit != 'true'
        run: sccache --show-stats

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"

      - name: Test wheel install
        run: |
          pip install bindings/python/dist/*.whl
          python3 -c "import smg; print('Python package: OK')"
          python3 -c "from smg.smg_rs import Router; print('Rust extension: OK')"
          python3 -m smg.launch_router --help > /dev/null && echo "Entry point: OK"

  python-unit-tests:
    needs: build-wheel
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: dist/

      - name: Install wheel
        run: pip install dist/*.whl

      - name: Run Python unit tests
        run: |
          cd bindings/python
          python3 -m pip install pytest pytest-cov pytest-xdist
          pytest -q tests --cov=smg --cov-config=.coveragerc --cov-report=term-missing --cov-fail-under=80

  unit-tests:
    needs: check-ci
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Setup Rust
        uses: ./.github/actions/setup-rust

      - name: Run lint
        run: |
          source "$HOME/.cargo/env"
          rustup component add clippy
          cargo clippy --all-targets --all-features -- -D warnings

      - name: Run fmt
        run: |
          source "$HOME/.cargo/env"
          rustup component add --toolchain nightly-x86_64-unknown-linux-gnu rustfmt
          rustup toolchain install nightly --profile minimal
          cargo +nightly fmt -- --check

      - name: Generate vision golden fixtures
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

          pip install transformers pillow numpy scipy
          python multimodal/scripts/generate_vision_golden.py

      - name: Run Rust tests
        timeout-minutes: 30
        run: |
          source "$HOME/.cargo/env"
          cargo test

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats

  # =============================================================================
  # E2E Tests - Each runtime installs backend once, runs all tests sequentially
  # =============================================================================
  e2e-sglang:
    name: e2e-sglang
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 90
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install SGLang backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh pytest-parallel py

      - name: Setup Oracle Instant Client
        run: |
          sudo apt-get install -y unzip
          INSTANT_CLIENT_DIR="$HOME/instant-client"
          INSTANT_CLIENT_ZIP="instantclient-basic-linux.x64-23.9.0.25.07.zip"
          if [ ! -d "$INSTANT_CLIENT_DIR/instantclient_23_9" ]; then
            mkdir -p "$INSTANT_CLIENT_DIR"
            cd "$INSTANT_CLIENT_DIR"
            wget https://download.oracle.com/otn_software/linux/instantclient/2390000/$INSTANT_CLIENT_ZIP
            unzip $INSTANT_CLIENT_ZIP
            rm $INSTANT_CLIENT_ZIP
          fi
          echo "LD_LIBRARY_PATH=$HOME/instant-client/instantclient_23_9:$LD_LIBRARY_PATH" >> $GITHUB_ENV

      - name: Start Oracle Database
        run: |
          docker run -d -p 1521:1521 -e ORACLE_PASSWORD=oracle --name oracle-db gvenzl/oracle-xe:21-slim
          echo "ATP_USER=system" >> $GITHUB_ENV
          echo "ATP_PASSWORD=oracle" >> $GITHUB_ENV
          echo "ATP_DSN=localhost:1521/XEPDB1" >> $GITHUB_ENV

      - name: Start Brave MCP Server
        env:
          BRAVE_API_KEY: ${{ secrets.BRAVE_API_KEY }}
        run: |
          docker run -d --rm -p 8001:8080 -e BRAVE_API_KEY --name brave-search-server shoofio/brave-search-mcp-sse:1.0.10
          sleep 2

      - name: "Test: Smoke (router + embeddings)"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            e2e_test/router e2e_test/embeddings -s -vv -o log_cli=true --log-cli-level=INFO

      - name: "Test: Chat completions"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          E2E_RUNTIME=sglang SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            e2e_test/chat_completions -s -vv -o log_cli=true --log-cli-level=INFO

      - name: "Test: Responses (MCP)"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 \
            e2e_test/responses -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Cleanup
        if: always()
        run: |
          docker stop brave-search-server oracle-db || true
          docker rm brave-search-server oracle-db || true

  e2e-vllm:
    name: e2e-vllm
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 90
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Cache flash-attn wheel
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip/wheels
          key: flash-attn-${{ runner.os }}-${{ hashFiles('scripts/ci_install_vllm.sh') }}

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install vLLM backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_vllm.sh

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh pytest-parallel py

      - name: "Test: Smoke (router + embeddings)"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            e2e_test/router e2e_test/embeddings -s -vv -o log_cli=true --log-cli-level=INFO

      - name: "Test: Chat completions"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          E2E_RUNTIME=vllm SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            -k 'not (5-grpc or 2-None-grpc or multiple_choices)' \
            e2e_test/chat_completions -s -vv -o log_cli=true --log-cli-level=INFO

  e2e-trtllm:
    name: e2e-trtllm
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 120
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Restore TRT-LLM wheel cache
        id: trtllm-cache
        uses: actions/cache/restore@v4
        with:
          path: /tmp/trtllm-wheel
          key: trtllm-wheel-${{ runner.os }}-cuda13-v2

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install TRT-LLM backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_trtllm.sh

      - name: Save TRT-LLM wheel cache
        if: steps.trtllm-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: /tmp/trtllm-wheel
          key: trtllm-wheel-${{ runner.os }}-cuda13-v2

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh pytest-parallel py

      - name: "Test: Smoke (router + embeddings)"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            e2e_test/router e2e_test/embeddings -s -vv -o log_cli=true --log-cli-level=INFO

      - name: "Test: Chat completions"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          E2E_RUNTIME=trtllm SHOW_WORKER_LOGS=1 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 --workers 1 --tests-per-worker 4 \
            e2e_test/chat_completions -s -vv -o log_cli=true --log-cli-level=INFO

  # =============================================================================
  # Benchmarks - runs after all E2E tests pass
  # =============================================================================
  benchmarks:
    name: benchmarks
    needs: [check-ci, build-wheel, e2e-sglang, e2e-vllm, e2e-trtllm]
    if: |
      always() &&
      needs.check-ci.outputs.should_run == 'true' &&
      needs.e2e-sglang.result == 'success' &&
      needs.e2e-vllm.result == 'success' &&
      needs.e2e-trtllm.result == 'success'
    runs-on: k8s-runner-gpu
    timeout-minutes: 32
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install SGLang backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh genai-bench==0.0.3

      - name: Run benchmarks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --ignore=e2e_test/benchmarks/test_go_bindings_perf.py \
            e2e_test/benchmarks -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: success()
        uses: actions/upload-artifact@v6
        with:
          name: genai-bench-results-all-policies
          path: benchmark_**/

  go-unit-tests:
    name: go-unit-tests
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-cpu
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Install build tools
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Run Go unit tests
        run: |
          cd bindings/golang
          export CGO_ENABLED=1
          export CGO_LDFLAGS="-L$(pwd)/target/release"
          export LD_LIBRARY_PATH="$(pwd)/target/release:$LD_LIBRARY_PATH"
          go test -v ./...

  go-bindings-e2e:
    name: go-bindings-e2e
    needs: [check-ci, build-wheel]
    if: needs.check-ci.outputs.should_run == 'true'
    runs-on: k8s-runner-gpu
    timeout-minutes: 45
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python venv and SGLang
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh

      - name: Run Go OAI server E2E tests
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest --reruns 2 --reruns-delay 5 e2e_test/bindings_go -s -vv -o log_cli=true --log-cli-level=INFO

  go-bindings-benchmark:
    name: go-bindings-benchmark
    needs: [check-ci, build-wheel]
    if: false  # Disabled
    runs-on: k8s-runner-gpu
    timeout-minutes: 32
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install inference backend
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          bash scripts/ci_install_sglang.sh

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: bindings/golang/go.sum

      - name: Download Go FFI library
        uses: actions/download-artifact@v7
        with:
          name: go-ffi-library
          path: bindings/golang/target/release/

      - name: Verify Go FFI library
        run: ls -la bindings/golang/target/release/libsmg_go.*

      - name: Download wheel artifact
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl

      - name: Install test dependencies
        run: |
          python3 -m pip install pytest pytest-rerunfailures httpx openai grpcio grpcio-health-checking numpy
          python3 -m pip --no-cache-dir install --upgrade genai-bench==0.0.3

      - name: Run Go bindings benchmark
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"
          export CGO_LDFLAGS="-L$(pwd)/bindings/golang/target/release"
          export LD_LIBRARY_PATH="$(pwd)/bindings/golang/target/release:$LD_LIBRARY_PATH"
          SHOW_WORKER_LOGS=0 SHOW_ROUTER_LOGS=1 ROUTER_LOCAL_MODEL_PATH="/home/ubuntu/models" \
            pytest e2e_test/benchmarks/test_go_bindings_perf.py -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: success()
        uses: actions/upload-artifact@v6
        with:
          name: genai-bench-results-go-bindings
          path: benchmark_go_bindings/

  finish:
    needs: [check-ci, build-wheel, python-unit-tests, unit-tests, e2e-sglang, e2e-vllm, e2e-trtllm, benchmarks, go-unit-tests, go-bindings-e2e]
    if: always()
    runs-on: k8s-runner-cpu
    permissions: {}
    steps:
      - name: Check CI result
        run: |
          if [[ "${{ needs.check-ci.outputs.should_run }}" != "true" ]]; then
            echo "CI was skipped (external contributor without run-ci label)"
            exit 0
          elif [[ "${{ needs.build-wheel.result }}" == "failure" || "${{ needs.unit-tests.result }}" == "failure" || "${{ needs.e2e-sglang.result }}" == "failure" || "${{ needs.e2e-vllm.result }}" == "failure" || "${{ needs.e2e-trtllm.result }}" == "failure" || "${{ needs.benchmarks.result }}" == "failure" || "${{ needs.go-unit-tests.result }}" == "failure" || "${{ needs.go-bindings-e2e.result }}" == "failure" ]]; then
            echo "One or more jobs failed"
            exit 1
          else
            echo "All jobs completed successfully"
          fi

  summarize-benchmarks:
    needs: [benchmarks]
    runs-on: k8s-runner-cpu
    if: success()
    permissions:
      contents: read
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: "3.13"

    - name: Download gateway benchmark results
      uses: actions/download-artifact@v7
      with:
        name: genai-bench-results-all-policies

    - name: Create benchmark summary
      run: python3 e2e_test/benchmarks/summarize.py .
