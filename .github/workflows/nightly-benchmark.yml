name: Nightly Benchmark

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      models:
        description: 'Comma-separated model list (default: all)'
        required: false
        default: 'all'
      runtime:
        description: 'Runtime: sglang, vllm, or all (default: all)'
        required: false
        default: 'all'

concurrency:
  group: nightly-benchmark
  cancel-in-progress: false

permissions:
  contents: read

jobs:
  # ---------------------------------------------------------------------------
  # Build wheel once and reuse across all jobs
  # ---------------------------------------------------------------------------

  build-wheel:
    runs-on: k8s-runner-cpu
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Rust
        uses: ./.github/actions/setup-rust

      - name: Setup Python venv
        run: bash scripts/ci_setup_python_venv.sh

      - name: Build Python wheel
        run: bash scripts/ci_build_wheel.sh

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v6
        with:
          name: smg-wheel
          path: bindings/python/dist/*.whl
          retention-days: 7

  # ---------------------------------------------------------------------------
  # Single-worker: one model per job for better isolation and debugging.
  # DeepSeek runs on H200; all other models run on k8s H100.
  # ---------------------------------------------------------------------------

  single-worker:
    name: "${{ matrix.model.id }} / single-${{ matrix.variant.id }}"
    needs: build-wheel
    if: ${{ !cancelled() }}
    runs-on: ${{ fromJson(matrix.model.runs_on) }}
    timeout-minutes: 1440
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        model:
          - { id: meta-llama/Llama-3.1-8B-Instruct, slug: meta-llama-Llama-3.1-8B-Instruct, test_class: TestNightlyLlama8bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: meta-llama/Llama-3.2-1B-Instruct, slug: meta-llama-Llama-3.2-1B-Instruct, test_class: TestNightlyLlama1bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: Qwen/Qwen2.5-7B-Instruct, slug: Qwen-Qwen2.5-7B-Instruct, test_class: TestNightlyQwen7bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: Qwen/Qwen2.5-14B-Instruct, slug: Qwen-Qwen2.5-14B-Instruct, test_class: TestNightlyQwen14bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, slug: deepseek-ai-DeepSeek-R1-Distill-Qwen-7B, test_class: TestNightlyDeepseek7bSingle, runs_on: '["8-gpu-h200"]', gpu_type: H200 }
          - { id: Qwen/Qwen3-30B-A3B, slug: Qwen-Qwen3-30B-A3B, test_class: TestNightlyQwen30bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: mistralai/Mistral-7B-Instruct-v0.3, slug: mistralai-Mistral-7B-Instruct-v0.3, test_class: TestNightlyMistral7bSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: openai/gpt-oss-20b, slug: openai-gpt-oss-20b, test_class: TestNightlyGptOssSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8, slug: meta-llama-Llama-4-Maverick-17B-128E-Instruct-FP8, test_class: TestNightlyLlama4MaverickSingle, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
        variant:
          - { id: sglang, runtime: sglang, grpc_only: "false" }
          - { id: vllm,   runtime: vllm,   grpc_only: "true" }

    steps:
      - name: Check filters
        id: filter
        run: |
          MODELS="${{ github.event.inputs.models || 'all' }}"
          RUNTIME="${{ github.event.inputs.runtime || 'all' }}"
          SKIP="false"
          if [ "$MODELS" != "all" ] && ! echo ",$MODELS," | grep -Fq ",${{ matrix.model.id }},"; then
            SKIP="true"
          fi
          if [ "$RUNTIME" != "all" ] && [ "$RUNTIME" != "${{ matrix.variant.id }}" ]; then
            SKIP="true"
          fi
          echo "skip=$SKIP" >> "$GITHUB_OUTPUT"

      - name: Checkout code
        if: steps.filter.outputs.skip != 'true'
        uses: actions/checkout@v6

      - name: Install inference backend
        if: steps.filter.outputs.skip != 'true'
        env:
          SGLANG_USE_LATEST_TAG: "1"
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          if [ "${{ matrix.variant.runtime }}" == "vllm" ]; then
            bash scripts/ci_install_vllm.sh
          else
            bash scripts/ci_install_sglang.sh
          fi

      - name: Download wheel artifact
        if: steps.filter.outputs.skip != 'true'
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        if: steps.filter.outputs.skip != 'true'
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh genai-bench

      - name: Run benchmark
        if: steps.filter.outputs.skip != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          GPU_TYPE: ${{ matrix.model.gpu_type }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"

          K_FILTER="${{ matrix.model.test_class }}"
          if [ "${{ matrix.variant.grpc_only }}" == "true" ]; then
            K_FILTER="${{ matrix.model.test_class }} and grpc"
          fi

          E2E_RUNTIME=${{ matrix.variant.runtime }} \
          ROUTER_LOCAL_MODEL_PATH="/raid/models" \
            pytest e2e_test/benchmarks/test_nightly_perf.py \
              -k "$K_FILTER" \
              -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: always() && steps.filter.outputs.skip != 'true'
        uses: actions/upload-artifact@v6
        with:
          name: nightly-${{ matrix.model.slug }}-single-${{ matrix.variant.id }}-${{ github.run_id }}
          path: nightly_*/
          retention-days: 90

      - name: Cleanup
        if: always() && steps.filter.outputs.skip != 'true'
        run: bash scripts/ci_killall_sglang.sh || true

  # ---------------------------------------------------------------------------
  # Multi-worker: one model per job.
  # DeepSeek runs on H200; all other models run on k8s H100.
  # ---------------------------------------------------------------------------

  multi-worker:
    name: "${{ matrix.model.id }} / multi-${{ matrix.variant.id }}"
    needs: build-wheel
    if: ${{ !cancelled() }}
    runs-on: ${{ fromJson(matrix.model.runs_on) }}
    timeout-minutes: 1440
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        model:
          - { id: meta-llama/Llama-3.1-8B-Instruct, slug: meta-llama-Llama-3.1-8B-Instruct, test_class: TestNightlyLlama8bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: meta-llama/Llama-3.2-1B-Instruct, slug: meta-llama-Llama-3.2-1B-Instruct, test_class: TestNightlyLlama1bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: Qwen/Qwen2.5-7B-Instruct, slug: Qwen-Qwen2.5-7B-Instruct, test_class: TestNightlyQwen7bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: Qwen/Qwen2.5-14B-Instruct, slug: Qwen-Qwen2.5-14B-Instruct, test_class: TestNightlyQwen14bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, slug: deepseek-ai-DeepSeek-R1-Distill-Qwen-7B, test_class: TestNightlyDeepseek7bMulti, runs_on: '["8-gpu-h200"]', gpu_type: H200 }
          - { id: Qwen/Qwen3-30B-A3B, slug: Qwen-Qwen3-30B-A3B, test_class: TestNightlyQwen30bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: mistralai/Mistral-7B-Instruct-v0.3, slug: mistralai-Mistral-7B-Instruct-v0.3, test_class: TestNightlyMistral7bMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          - { id: openai/gpt-oss-20b, slug: openai-gpt-oss-20b, test_class: TestNightlyGptOssMulti, runs_on: '["k8s-runner-gpu","4-gpu-h100"]', gpu_type: H100 }
          # - { id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8, test_class: TestNightlyLlama4MaverickMulti } # tp=8, keep disabled for nightly
        variant:
          - { id: sglang, runtime: sglang, grpc_only: "false" }
          - { id: vllm,   runtime: vllm,   grpc_only: "true" }

    steps:
      - name: Check filters
        id: filter
        run: |
          MODELS="${{ github.event.inputs.models || 'all' }}"
          RUNTIME="${{ github.event.inputs.runtime || 'all' }}"
          SKIP="false"
          if [ "$MODELS" != "all" ] && ! echo ",$MODELS," | grep -Fq ",${{ matrix.model.id }},"; then
            SKIP="true"
          fi
          if [ "$RUNTIME" != "all" ] && [ "$RUNTIME" != "${{ matrix.variant.id }}" ]; then
            SKIP="true"
          fi
          echo "skip=$SKIP" >> "$GITHUB_OUTPUT"

      - name: Checkout code
        if: steps.filter.outputs.skip != 'true'
        uses: actions/checkout@v6

      - name: Install inference backend
        if: steps.filter.outputs.skip != 'true'
        env:
          SGLANG_USE_LATEST_TAG: "1"
        run: |
          bash scripts/ci_setup_python_venv.sh
          source .venv/bin/activate
          if [ "${{ matrix.variant.runtime }}" == "vllm" ]; then
            bash scripts/ci_install_vllm.sh
          else
            bash scripts/ci_install_sglang.sh
          fi

      - name: Download wheel artifact
        if: steps.filter.outputs.skip != 'true'
        uses: actions/download-artifact@v7
        with:
          name: smg-wheel
          path: wheel/

      - name: Install wheel and test dependencies
        if: steps.filter.outputs.skip != 'true'
        run: |
          pip uninstall -y smg || true
          pip install wheel/*.whl
          bash scripts/ci_install_e2e_deps.sh genai-bench

      - name: Run benchmark
        if: steps.filter.outputs.skip != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          GPU_TYPE: ${{ matrix.model.gpu_type }}
        run: |
          bash scripts/ci_killall_sglang.sh "nuk_gpus"

          K_FILTER="${{ matrix.model.test_class }}"
          if [ "${{ matrix.variant.grpc_only }}" == "true" ]; then
            K_FILTER="${{ matrix.model.test_class }} and grpc"
          fi

          E2E_RUNTIME=${{ matrix.variant.runtime }} \
          ROUTER_LOCAL_MODEL_PATH="/raid/models" \
            pytest e2e_test/benchmarks/test_nightly_perf.py \
              -k "$K_FILTER" \
              -s -vv -o log_cli=true --log-cli-level=INFO

      - name: Upload benchmark results
        if: always() && steps.filter.outputs.skip != 'true'
        uses: actions/upload-artifact@v6
        with:
          name: nightly-${{ matrix.model.slug }}-multi-${{ matrix.variant.id }}-${{ github.run_id }}
          path: nightly_*/
          retention-days: 90

      - name: Cleanup
        if: always() && steps.filter.outputs.skip != 'true'
        run: bash scripts/ci_killall_sglang.sh || true

  # ---------------------------------------------------------------------------
  # Summary
  # ---------------------------------------------------------------------------

  summarize-benchmarks:
    needs: [single-worker, multi-worker]
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download all benchmark results
        uses: actions/download-artifact@v7
        with:
          pattern: nightly-*
          merge-multiple: true

      - name: Create benchmark summary
        run: python3 e2e_test/benchmarks/nightly_summarize.py .
